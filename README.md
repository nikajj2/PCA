Principal Component Analysis (PCA)

Principal Component Analysis (PCA) is a widely used method for analyzing large datasets with many dimensions. It simplifies data interpretation while preserving critical information and facilitating multidimensional data visualization. PCA reduces dataset dimensionality by transforming the data into a new coordinate system where most of the data's variation can be explained using fewer dimensions. By identifying principal components that capture significant variation, PCA provides a concise representation of the data.
	Karl Pearson introduced PCA in 1901, inspired by the principal axis theorem. Harold Hotelling independently developed and named it in the 1930s. PCA has diverse applications and is known by different names in various fields. In signal processing, it is called the discrete Karhunen-Loève transform (KLT), while in multivariate quality control, it is referred to as the Hotelling transform. In mechanical engineering, it is known as proper orthogonal decomposition (POD). The singular value decomposition (SVD) of X, a crucial part of PCA, was invented in the latter part of the 20th century. PCA's broad applications and various names highlight its significance in data analysis and dimensionality reduction. In practice, PCA often utilizes the first two principal components for visualizing data in two dimensions, aiding in the identification of clusters or patterns. This visualization technique helps explore relationships and structures within the dataset. 
PCA consists of five main steps. The first step in PCA is called “Standardization.” In order to ensure that each variable contributes equally to the analysis, it is crucial to normalize the range of the initial variables. This step is necessary because PCA is sensitive to the variances of the original variables. When the initial variables have different ranges, those variables with larger ranges can dominate the analysis and have a disproportionate influence on the results. For example, if one variable ranges from 0 to 100 and another variable ranges from 0 to 1, the variable with the larger range will have a greater impact on the principal components. Standardization brings all the variables to a common scale, typically by subtracting the mean and dividing by the standard deviation. 
This equalizes the influence of each variable and ensures that the principal components capture the overall structure and variation of the dataset more accurately. Performing standardization prior to PCA helps prevent biased results and ensures that the principal components represent the true underlying relationships within the data. 
	In the second step is computed the covariance matrix. The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. This step aims to assess the relationships and variations among variables in the input dataset. Its goal is to understand how variables vary with respect to each other and identify any existing relationships. Sometimes, variables are highly correlated and may contain redundant information. To detect these correlations, the covariance matrix is computed. The covariance matrix provides insights into the relationships between variables, helping to identify patterns and dependencies.  This knowledge allows for informed decisions regarding which variables to include or exclude in subsequent steps of the PCA analysis. 
	The third step consists of computing eigenvectors and eigenvalues of the covariance matrix to identify the principal components. Principal components are derived from the original variables by combining them in linear ways. This construction aims to create new variables, referred to as principal components, that are both uncorrelated and capable of capturing most of the information contained in the original variables. In essence, if there are 10 initial variables, PCA tries to put most of the information into the first component, followed by the maximum remaining information in the second component, and so forth. 
 By organizing information into principal components, it becomes possible to reduce dimensionality while retaining a significant amount of information. This is achieved by discarding components with low information and considering the remaining components as the new variables. From a geometric standpoint, the principal components represent the directions (eigenvectors) in the data that explain the highest amount of variance (eigenvalue). In other words, they are the lines that capture the most information in the data. The relationship between variance and information can be understood as follows: a line with a larger variance corresponds to a greater dispersion of data points along that line, indicating a higher level of information. In simpler terms, envision the principal components as new axes that provide an optimal perspective to visualize and assess the data, making the differences between observations more apparent. In order to find eigenvectors and eigenvalues is used QR algorithm which is an iterative numerical that decomposes a given matrix into the product of an orthogonal matrix (Q) and an upper triangular matrix (R) through a process called QR decomposition. By repeatedly applying the QR decomposition to a matrix, the algorithm converges toward the eigenvalues of the original matrix. Once the eigenvectors and eigenvalues are founded, pairs are put in descending order based on the eigenvalue and it’s time for step 4.
	In the fourth step, we must choose whether to keep all the components or discard those with lower eigenvalues. Then, with the components we decided to keep, we form a matrix of vectors called Feature vector.
	In the fifth step, the data is recast along the principal component axis. This is done by multiplying the transpose of the feature vector by the transpose of the original data set.
 PCA has been used in the Detection and Visualization of computer network attacks as well because it is applied for dimensionality reduction of data, anomaly detection in network traffic, extracting the important features of the network that are important for attack detection, for network traffic classification and for visualizing high dimension network attacks.
	In medicine, PCA has very high usage. Here we can mention the use of PCA in disease diagnosis and classification, especially cancer and other chronic diseases. It also plays a crucial role in genetics research, anomaly detection, image processing like MRI, CT scans, and PET scans, drug discovery and development, patient monitoring, health assessment, etc. It is also used in Medical Data to show the correlation of Cholesterol with low-density lipoprotein. Furthermore, in neuroscience, a technique known as spike-triggered covariance analysis uses a variant of Principal Components Analysis to identify the specific properties of a stimulus that increase a neuron's probability of generating an action potential. PCA is also used to find the identity of a neuron from the shape of its action potential and to detect coordinated activities of large neuronal ensembles. It is used in determining collective variables during phase transitions in the brain as well.
	Another use of PCA is in quantitative finance. Suppose a fund manager has 200 stocks in his portfolio. To analyze these stocks quantitatively a stock manager will require a co-relational matrix of the size 200 * 200, which makes the problem very complex. However, if he was to extract, 10 Principal Components which best represent the variance in the stocks, this would reduce the complexity of the problem while still explaining the movement of all 200 stocks. Some other applications of PCA in finance include analyzing the shape of the yield curve, hedging fixed-income portfolios, implementing interest rate models, forecasting portfolio returns, developing asset allocation algorithms, and developing long-short equity trading algorithms.
	 Even though PCA is widely used, PCA has several disadvantages that need to be considered. First, interpretability can be challenging as it may be difficult to relate the resulting principal components back to the original features. Additionally, there is a risk of information loss if crucial features are not well-represented in the selected components, potentially leading to a loss of important insights. PCA is also vulnerable to outliers, as they can significantly impact the resulting components and distort the analysis. Another limitation is the assumption of properly scaled and centralized data, which may affect the accuracy of the components if this assumption is violated. Finally, the computational complexity of computing eigenvectors and eigenvalues for large datasets can be demanding, which limits the scalability of PCA for certain applications.
In conclusion, despite its limitations, PCA remains a powerful and valuable method in various domains. It provides valuable insights and facilitates efficient data analysis. Researchers and practitioners persist in exploring and leveraging the strengths of PCA while addressing its limitations, deriving meaningful outcomes from complex datasets.
